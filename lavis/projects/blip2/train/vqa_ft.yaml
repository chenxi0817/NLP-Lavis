# =================================================================================
# FINAL & COMPLETE: BLIP-2 with OPT 2.7B - VQA Fine-tuning Configuration
# =================================================================================
model:
  arch: blip2_opt
  model_type: pretrain_opt2.7b
  
  # 从官方预训练权重开始
  pretrained: "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth"
  
  # PEFT策略：只训练Q-Former，以保证稳定性
  freeze_vit: True
  freeze_qformer: False  # 关键：确保Q-Former是可训练的

datasets:
  # 直接使用为VQA设计的coco_vqa数据集
  coco_vqa:
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
      eval:
        name: "blip_image_eval"
        image_size: 224
        
    text_processor:
      train:
        name: "blip_question"
        # 推荐在训练时也加入prompt，统一范式
        prompt: "Question: {} Short answer:"
      eval:
        name: "blip_question"

run:
  # 使用 'vqa' 任务 (我们已经通过修改vqa.py修复了它)
  task: vqa

  # 关键：指定使用生成模式，这将触发 valid_step 中的 if 分支
  inference_method: "generate"
  
  # 优化器和学习率调度器
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-4
  min_lr: 0
  warmup_lr: 1e-8
  warmup_steps: 1000
  weight_decay: 0.05
  
  # 训练过程控制
  max_epoch: 5
  batch_size_train: 32
  batch_size_eval: 32
  num_workers: 4
  accum_grad_iters: 1
  
  # 之前遗漏的基础参数
  seed: 42
  resume_ckpt_path: null
  output_dir: "output/BLIP2/VQA_finetune"

  # 数据集切片配置
  # 关键：明确告知这是训练任务，而非仅评估任务
  evaluate: False
  train_splits: ["train"]
  valid_splits: ["val"]

  # 分布式训练配置
  device: "cuda"
  world_size: 3
  dist_url: "env://"
  distributed: True